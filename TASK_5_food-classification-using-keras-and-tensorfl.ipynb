{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://th.bing.com/th/id/R.fa6afa266f7707ecb9a373c21cfe3fe8?rik=%2bGUmDE6I3Hd1Vw&pid=ImgRaw&r=0?raw=true\" width=\"2400\"><img src=\"/kaggle/input/photo-of-dogcat/OIP.jpg\" alt=\"Banner\" style=\"width: 100%; height: auto;\"/>\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: gainsboro; font-size:130%; text-align:left\">\n<h1 align=\"left\"><font color=#0a1f89>Description:</font></h1>  \nThe project aims to develop a deep learning model capable of accurately recognizing food items from images and estimating their calorie content. With the prevalence of smartphones and the increasing interest in health and nutrition, such a model could empower users to track their dietary intake more effectively, make informed food choices, and achieve their health and fitness goals. By leveraging advanced machine learning techniques, including convolutional neural networks (CNNs) for image recognition and regression algorithms for calorie estimation, the model seeks to provide users with a convenient and accessible tool for managing their nutrition.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: gainsboro; font-size:130%; text-align:left\">\n\n<h1 align=\"left\"><font color=#0a1f89>Objectives:</font></h1>    \n    \n    \n- Download and extract Food 101 dataset.\n- Understand dataset structure and files.\n- Visualize random image from each of the 101 classes.\n- Split the image data into train and test using train.txt and test.txt.\n- Create a subset of data with few classes(3) - train_mini and test_mini for experimenting.\n- Fine tune Inception Pretrained model using Food 101 dataset.\n- Visualize accuracy and loss plots.\n- Predicting classes for new images from internet.\n- Scale up and fine tune Inceptionv3 model with 11 classes of data.\n- Model Explainability.\n- Summary of the things I tried.\n- Further improvements.\n- Feedback.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:130%; text-align:left\">\n\n<h1 align=\"left\"><font color=#0a1f89>Applications </font></h1>\n    \n- **Dietary Tracking**: Users can track their daily food intake by simply taking photos of their meals, enabling them to monitor their calorie consumption and make adjustments to their diet as needed.\n    \n- **Health and Fitness Monitoring**: The model can provide valuable insights into users' nutritional habits, helping them make healthier food choices and achieve their fitness goals.\n\n- **Weight Management**: By accurately estimating the calorie content of meals, the model can assist individuals in managing their weight more effectively and maintaining a healthy lifestyle.\n\n- **Nutritional Education**: The model can serve as an educational tool by providing users with information about the nutritional content of various foods, fostering greater awareness of dietary choices and their impact on health.\n    \n- **Personalized Recommendations**: Over time, the model can learn users' preferences and dietary patterns, providing personalized recommendations for balanced meals and optimal nutrition.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"contents_tabel\"></a>    \n<div style=\"border-radius:10px; padding: 15px; background-color: gainsboro; font-size:130%; text-align:left\">\n\n<h1 align=\"left\"><font color=#0a1f89>Table of Contents:</font></h1>\n    \n    \n* [Step 1 | Setup and Initialization](#Initialization)\n    - [Step 1.1 | Importing Libraries](#Libraries)\n    - [Step 1.2 | Checking State of GPU](#GPU)\n    - [Step 1.3 | Changing Directory](#Directory)\n    - [Step 1.4 | Downloading and Extracting the Dataset](#DownloadingData)\n      \n* [Step 2 | Data Exploration and Verification](#DEV)\n    - [Step 2.1 | Understanding the Structure of Image Data](#UnderstandingData)\n    - [Step 2.2 | Understanding the Metadata or Additional Information](#MetaData)   \n    \n* [Step 3 | Data Visualization](#DataVisual)\n    \n* [Step 4 | Data Preprocessing](#DataProcessing)\n    - [Step 4.1 | Data Splitting for Training and Test](#Splitting)\n    - [Step 4.2 | Prepares the Train Dataset](#TrainData)\n    - [Step 4.3 | Creating the Test Data](#TestData)\n    - [Step 4.4 | Counting the Files and Directories in \"Train\" Folder](#TrainCount) \n    - [Step 4.5 | Counting the Files and Directories in \"Test\" Folder](#TestCount)\n    - [Step 4.6 | Removing the .DS_Store Entry](#DS_Store) \n    - [Step 4.7 | Creaing Subset](#CreaingSubset) \n    \n\n* [Step 5 | Training Neural Network Model for Image Classification](#TrainingNeuralNetwork)\n    \n* [Fine tune Inception Pretrained model using Food 101 dataset](#FinetuneInception1) \n    \n    - [Step 5.1 | Obtaining the Class Indices Mapping ](#IndicesMapping)\n    - [Step 5.2 | Visualizing the Training and Validation Accuracy](#VisualizingTrain/Validation)\n    - [Step 5.3 | Model Evaluation](#ModelEvaluation)\n    - [Step 5.4 | Predicting Class Label](#PredictingClassLabel) \n    - [Step 5.5 | Downloading Images from the Internet](#DownloadImages)\n    \n    \n   \n* [Fine tune Inceptionv3 model with 11 classes of data](#FinetuneInception)    \n    \n* [Step 6 | Model Explainability](#ModelExplainability)\n    \n* [Step 7 | Evaluating the Model](#EvaluateModel)\n    \n    - [Step 7.1 | Loading the Saved Model and a Test Image](#LoadingtheSavedModel)\n    - [Step 7.2 | Summary of the Model](#SummaryModel)\n    - [Step 7.3 | Defining Helper Functions](#DefiningHelperFunctions)\n    - [Step 7.4 | Generating Pattern Function](#GeneratingPatternFunction) \n    - [Step 7.5 | Getting Activations Function](#GettingActivationsFunction)\n    - [Step 7.6 | Showing Activations Function](#ShowingActivationsFunction) \n    - [Step 7.7 | Extracting Intermediate Layer Activations](#ExtractingIntermediateLayerActivations) \n    - [Step 7.8 | Extracting Layer Names for Intermediate Activations](#ExtractingLayerNamesforIntermediateActivations)  \n    - [Step 7.9 | Getting the Activations for a Different Input / Food](#GettingtheActivationsforaDifferentInput/Food) \n    - [Step 7.10 | Look into the Sparse Activations in the Layer Activation_1](#LookintotheSparseActivationsintheLayerActivation_1/Food)   \n    - [Step 7.11 | Visualization of Convolutional Layer Activations](#VisualizationofConvolutionalLayerActivations)     \n    - [Step 7.12 | Generating Class Activation Map (CAM)](#GeneratingClassActivationMap)     \n    - [Step 7.13 | Getting Attribution and Display Softmax Predictions](#GettingAttributionandDisplaySoftmaxPredictions)     \n    - [Step 7.14 | Downloading Images from URLs](#DownloadingImagesfromURLs)                 \n    - [Step 7.15 | Loading and Retrieving Activations](#LoadingandRetrievingActivations)               \n    \n    \n    \n    \n   ","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0c741c>Let's get started:</font></h2>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Initialization\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 1 | Setup and Initialization</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Libraries\"></a>\n# <b><span style='color:black'>Step 1.1 |</span><span style='color:#742d0c '> Importing Libraries</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf  # Import TensorFlow library for deep learning tasks\nimport matplotlib.image as img  # Import matplotlib for image reading\nimport numpy as np  # Import NumPy for numerical operations\nfrom collections import defaultdict  # Import defaultdict for creating dictionaries with default values\nimport collections  # Import collections module for collection data types\nfrom shutil import copy  # Import shutil for high-level file operations\nfrom shutil import copytree, rmtree  # Import shutil for directory copying and removal\nimport tensorflow.keras.backend as K  # Import Keras backend functions\nfrom tensorflow.keras.models import load_model  # Import Keras function for loading pre-trained models\nfrom tensorflow.keras.preprocessing import image  # Import Keras for image preprocessing\nimport matplotlib.pyplot as plt  # Import matplotlib for visualization\nimport os  # Import os module for operating system functions\nimport random  # Import random module for generating random numbers\nimport cv2  # Import OpenCV for image processing\nfrom tensorflow.keras import regularizers  # Import regularizers for regularization techniques\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3  # Import pre-trained InceptionV3 model\nfrom tensorflow.keras.models import Sequential, Model  # Import Sequential and Model for building neural network models\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten  # Import layers for building neural network architectures\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D  # Import layers for building convolutional neural networks\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator  # Import ImageDataGenerator for data augmentation\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger  # Import callbacks for model saving and logging\nfrom tensorflow.keras.optimizers import SGD  # Import SGD optimizer for training models\nfrom tensorflow.keras.regularizers import l2  # Import L2 regularization\nfrom tensorflow import keras  # Import Keras for deep learning tasks\nfrom tensorflow.keras import models  # Import Keras for building neural network models\nimport zipfile\nimport os\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:34.258407Z","iopub.execute_input":"2024-02-15T14:05:34.259141Z","iopub.status.idle":"2024-02-15T14:05:34.270382Z","shell.execute_reply.started":"2024-02-15T14:05:34.259109Z","shell.execute_reply":"2024-02-15T14:05:34.269091Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GPU\"></a>\n# <b><span style='color:black'>Step 1.2 |</span><span style='color:#742d0c '> Checking State of GPU</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Check TensorFlow version\nprint(tf.__version__)\n\n# Check GPU device name\nprint(tf.test.gpu_device_name())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:34.290166Z","iopub.execute_input":"2024-02-15T14:05:34.290459Z","iopub.status.idle":"2024-02-15T14:05:34.296797Z","shell.execute_reply.started":"2024-02-15T14:05:34.290435Z","shell.execute_reply":"2024-02-15T14:05:34.295916Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"2.15.0\n/device:GPU:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"Directory\"></a>\n# <b><span style='color:black'>Step 1.3 |</span><span style='color:#742d0c '> Changing Directory</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/input/food-101/","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:34.330476Z","iopub.execute_input":"2024-02-15T14:05:34.330989Z","iopub.status.idle":"2024-02-15T14:05:34.337803Z","shell.execute_reply.started":"2024-02-15T14:05:34.330963Z","shell.execute_reply":"2024-02-15T14:05:34.336935Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"/kaggle/input/food-101\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"DownloadingData\"></a>\n# <b><span style='color:black'>Step 1.4 |</span><span style='color:#742d0c '> Downloading and Extracting the Dataset</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"def get_data_extract():\n    \"\"\"\n    Check if the dataset exists, and if not, download and extract it.\n    \"\"\"\n    if \"food-101\" in os.listdir():\n        print(\"Dataset already exists\")\n    else:\n        print(\"Downloading the data...\")\n        !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n        print(\"Dataset downloaded!\")\n        print(\"Extracting data..\")\n        !tar xzvf food-101.tar.gz\n        print(\"Extraction done!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:34.367637Z","iopub.execute_input":"2024-02-15T14:05:34.367904Z","iopub.status.idle":"2024-02-15T14:05:34.375405Z","shell.execute_reply.started":"2024-02-15T14:05:34.367882Z","shell.execute_reply":"2024-02-15T14:05:34.374489Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Download data and extract it to folder\n# Uncomment this below line if you are on Colab\n\nget_data_extract()","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:34.383459Z","iopub.execute_input":"2024-02-15T14:05:34.383711Z","iopub.status.idle":"2024-02-15T14:05:34.388114Z","shell.execute_reply.started":"2024-02-15T14:05:34.383689Z","shell.execute_reply":"2024-02-15T14:05:34.387364Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Dataset already exists\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"DEV\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 2 | Data Exploration and Verification</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Check the extracted dataset folder\n!ls food-101/","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:34.426354Z","iopub.execute_input":"2024-02-15T14:05:34.426616Z","iopub.status.idle":"2024-02-15T14:05:35.386843Z","shell.execute_reply.started":"2024-02-15T14:05:34.426594Z","shell.execute_reply":"2024-02-15T14:05:35.385782Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"__MACOSX  food-101\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n- The dataset being used is **[Food 101](https://www.vision.ee.ethz.ch/datasets_extra/food-101/)**\n- This dataset has 101000 images in total. It's a food dataset with 101 categories(multiclass)\n- Each type of food has 750 training samples and 250 test samples\n- Note found on the webpage of the dataset :  \n- On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. \n- All images were rescaled to have a maximum side length of 512 pixels.\n- The entire dataset is 5GB in size","metadata":{}},{"cell_type":"markdown","source":"<a id=\"UnderstandingData\"></a>\n# <b><span style='color:black'>Step 2.1 |</span><span style='color:#742d0c '> Understanding the Structure of Image Data</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"os.listdir('food-101/images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"MetaData\"></a>\n# <b><span style='color:black'>Step 2.2 |</span><span style='color:#742d0c '> Understanding the Metadata or Additional Information</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\nUnderstanding the Metadata or Additional Information","metadata":{}},{"cell_type":"code","source":"os.listdir('food-101/meta')","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.439926Z","iopub.status.idle":"2024-02-15T14:05:35.440289Z","shell.execute_reply.started":"2024-02-15T14:05:35.440096Z","shell.execute_reply":"2024-02-15T14:05:35.440110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head food-101/meta/train.txt\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.441928Z","iopub.status.idle":"2024-02-15T14:05:35.442303Z","shell.execute_reply.started":"2024-02-15T14:05:35.442107Z","shell.execute_reply":"2024-02-15T14:05:35.442121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head food-101/meta/classes.txt","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.443218Z","iopub.status.idle":"2024-02-15T14:05:35.443582Z","shell.execute_reply.started":"2024-02-15T14:05:35.443419Z","shell.execute_reply":"2024-02-15T14:05:35.443433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"DataVisual\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 3 | Data Visualization</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Define the number of rows and columns for the subplot grid\nrows = 17\ncols = 6\n\n# Create a subplot grid with specified size\nfig, ax = plt.subplots(rows, cols, figsize=(25,25))\n\n# Set the title of the plot\nfig.suptitle(\"Showing one random image from each class\", y=1.05, fontsize=24)\n\n# Define the directory containing the image data\ndata_dir = \"food-101/images/\"\n\n# Get a sorted list of food class names\nfoods_sorted = sorted(os.listdir(data_dir))\n\n# Initialize food_id variable\nfood_id = 0\n\n# Loop through rows and columns to display images\nfor i in range(rows):\n    for j in range(cols):\n        try:\n            food_selected = foods_sorted[food_id] \n            food_id += 1\n        except:\n            break\n        if food_selected == '.DS_Store':\n            continue\n        # Get a list of images for the current food class\n        food_selected_images = os.listdir(os.path.join(data_dir, food_selected))\n        # Select a random image from the list\n        food_selected_random = np.random.choice(food_selected_images)\n        # Read and display the image\n        img = plt.imread(os.path.join(data_dir, food_selected, food_selected_random))\n        ax[i][j].imshow(img)\n        ax[i][j].set_title(food_selected, pad=10)  # Set the title of the subplot\n        \n# Remove x and y ticks from all subplots\nplt.setp(ax, xticks=[], yticks=[])\n\n# Adjust the layout of subplots to fit the figure\nplt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.445351Z","iopub.status.idle":"2024-02-15T14:05:35.445679Z","shell.execute_reply.started":"2024-02-15T14:05:35.445517Z","shell.execute_reply":"2024-02-15T14:05:35.445530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"DataProcessing\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 4 | Data Preprocessing</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Splitting\"></a>\n# <b><span style='color:black'>Step 4.1 |</span><span style='color:#742d0c '> Data Splitting for Training and Test</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src, dest):\n    # Create a dictionary to store image paths for each class\n    classes_images = defaultdict(list)\n    \n    # Read the filepath and extract image paths\n    with open(filepath, 'r') as txt:\n        paths = [read.strip() for read in txt.readlines()]\n        for p in paths:\n            food = p.split('/')\n            classes_images[food[0]].append(food[1] + '.jpg')\n\n    # Iterate over classes and copy images to destination folder\n    for food in classes_images.keys():\n        print(\"\\nCopying images into \", food)\n        if not os.path.exists(os.path.join(dest, food)):\n            os.makedirs(os.path.join(dest, food))\n        for i in classes_images[food]:\n            copy(os.path.join(src, food, i), os.path.join(dest, food, i))\n    print(\"Copying Done!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.447116Z","iopub.status.idle":"2024-02-15T14:05:35.447599Z","shell.execute_reply.started":"2024-02-15T14:05:35.447362Z","shell.execute_reply":"2024-02-15T14:05:35.447383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TrainData\"></a>\n# <b><span style='color:black'>Step 4.2 |</span><span style='color:#742d0c '> Prepares the Train Dataset </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Change current directory to the root directory\n%cd /\n\n# Print message indicating the start of creating train data\nprint(\"Creating train data...\")\n\n# Call prepare_data function to copy images from train.txt to train directory\nprepare_data('/kaggle/input/food-101/food-101/meta/train.txt', '/kaggle/input/food-101/food-101/images', 'train')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.449957Z","iopub.status.idle":"2024-02-15T14:05:35.450661Z","shell.execute_reply.started":"2024-02-15T14:05:35.450407Z","shell.execute_reply":"2024-02-15T14:05:35.450427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TestData\"></a>\n# <b><span style='color:black'>Step 4.3 |</span><span style='color:#742d0c '> Creating the Test Data </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Print message indicating the start of creating test data\nprint(\"Creating test data...\")\n\n# Call prepare_data function to copy images from test.txt to test directory\nprepare_data('/kaggle/input/food-101/food-101/meta/test.txt', '/kaggle/input/food-101/food-101/images', 'test')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.451795Z","iopub.status.idle":"2024-02-15T14:05:35.452286Z","shell.execute_reply.started":"2024-02-15T14:05:35.452029Z","shell.execute_reply":"2024-02-15T14:05:35.452048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TrainCount\"></a>\n# <b><span style='color:black'>Step 4.4 |</span><span style='color:#742d0c '> Counting the Files and Directories in \"Train\" Folder </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Print message indicating the total number of samples in the train folder\nprint(\"Total number of samples in train folder\")\n\n# Execute the find command to search for files and directories in the train folder\n# -type d: Search for directories\n# -or: Logical OR operator\n# -type f: Search for regular files\n# -printf '.': Print a single character for each file or directory found\n# wc -c: Count the number of characters (which corresponds to the number of files and directories)\n!find train -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.454072Z","iopub.status.idle":"2024-02-15T14:05:35.454535Z","shell.execute_reply.started":"2024-02-15T14:05:35.454303Z","shell.execute_reply":"2024-02-15T14:05:35.454322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TestCount\"></a>\n# <b><span style='color:black'>Step 4.5 |</span><span style='color:#742d0c '> Counting the Files and Directories in \"Test\" Folder </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Print message indicating the total number of samples in the test folder\nprint(\"Total number of samples in test folder\")\n\n# Execute the find command to search for files and directories in the test folder\n# -type d: Search for directories\n# -or: Logical OR operator\n# -type f: Search for regular files\n# -printf '.': Print a single character for each file or directory found\n# wc -c: Count the number of characters (which corresponds to the number of files and directories)\n!find test -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.455744Z","iopub.status.idle":"2024-02-15T14:05:35.456200Z","shell.execute_reply.started":"2024-02-15T14:05:35.455961Z","shell.execute_reply":"2024-02-15T14:05:35.455979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n- We now have train and test data ready  \n- But to experiment and try different architectures, working on the whole data with 101 classes takes a lot of time and computation  \n- To proceed with further experiments, I am creating train_min and test_mini, limiting the dataset to 3 classes  \n- Since the original problem is multiclass classification which makes key aspects of architectural decisions different from that of binary classification, choosing 3 classes is a good start instead of 2","metadata":{}},{"cell_type":"markdown","source":"<a id=\"DS_Store\"></a>\n# <b><span style='color:black'>Step 4.6 |</span><span style='color:#742d0c '> Removing the .DS_Store Entry </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# List of all 101 types of foods(sorted alphabetically)\ndel foods_sorted[0] # remove .DS_Store from the list","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.457894Z","iopub.status.idle":"2024-02-15T14:05:35.458549Z","shell.execute_reply.started":"2024-02-15T14:05:35.458302Z","shell.execute_reply":"2024-02-15T14:05:35.458325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(foods_sorted)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.459958Z","iopub.status.idle":"2024-02-15T14:05:35.460415Z","shell.execute_reply.started":"2024-02-15T14:05:35.460169Z","shell.execute_reply":"2024-02-15T14:05:35.460187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"CreaingSubset\"></a>\n# <b><span style='color:black'>Step 4.7 |</span><span style='color:#742d0c '> Creaing Subset </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Helper method to create train_mini and test_mini data samples\ndef dataset_mini(food_list, src, dest):\n    # Check if the destination directory exists\n    if os.path.exists(dest):\n        # If it exists, remove it to ensure a clean slate\n        rmtree(dest)  # Removing dataset_mini (if it already exists) folders\n    # Create the destination directory\n    os.makedirs(dest)\n    \n    # Iterate over each food item in the provided list\n    for food_item in food_list:\n        print(\"Copying images into\", food_item)\n        # Recursively copy the images from the source directory to the destination directory for each food item\n        copytree(os.path.join(src, food_item), os.path.join(dest, food_item))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.461563Z","iopub.status.idle":"2024-02-15T14:05:35.461997Z","shell.execute_reply.started":"2024-02-15T14:05:35.461767Z","shell.execute_reply":"2024-02-15T14:05:35.461785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of food items for creating mini datasets\nfood_list = ['apple_pie', 'pizza', 'omelette']\n\n# Source and destination directories for train and test datasets\nsrc_train = 'train'\ndest_train = 'train_mini'\nsrc_test = 'test'\ndest_test = 'test_mini'\n\n# Create train_mini dataset\ndataset_mini(food_list, src_train, dest_train)\n\n# Create test_mini dataset\ndataset_mini(food_list, src_test, dest_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.463194Z","iopub.status.idle":"2024-02-15T14:05:35.463660Z","shell.execute_reply.started":"2024-02-15T14:05:35.463429Z","shell.execute_reply":"2024-02-15T14:05:35.463449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print message indicating the creation of the train data folder with new classes\nprint(\"Creating train data folder with new classes\")\n\n# Create train_mini dataset with specified food classes\ndataset_mini(food_list, src_train, dest_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.464784Z","iopub.status.idle":"2024-02-15T14:05:35.465231Z","shell.execute_reply.started":"2024-02-15T14:05:35.464999Z","shell.execute_reply":"2024-02-15T14:05:35.465019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print message indicating the total number of samples in the train folder\nprint(\"Total number of samples in train folder\")\n\n# Execute the find command to search for files and directories in the train_mini folder\n# -type d: Search for directories\n# -or: Logical OR operator\n# -type f: Search for regular files\n# -printf '.': Print a single character for each file or directory found\n# wc -c: Count the number of characters (which corresponds to the number of files and directories)\n!find train_mini -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.467002Z","iopub.status.idle":"2024-02-15T14:05:35.467472Z","shell.execute_reply.started":"2024-02-15T14:05:35.467215Z","shell.execute_reply":"2024-02-15T14:05:35.467233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print message indicating the creation of the test data folder with new classes\nprint(\"Creating test data folder with new classes\")\n\n# Create test_mini dataset with specified food classes\ndataset_mini(food_list, src_test, dest_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.468567Z","iopub.status.idle":"2024-02-15T14:05:35.468911Z","shell.execute_reply.started":"2024-02-15T14:05:35.468746Z","shell.execute_reply":"2024-02-15T14:05:35.468760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print message indicating the total number of samples in the test folder\nprint(\"Total number of samples in test folder\")\n\n# Execute the find command to search for files and directories in the test_mini folder\n# -type d: Search for directories\n# -or: Logical OR operator\n# -type f: Search for regular files\n# -printf '.': Print a single character for each file or directory found\n# wc -c: Count the number of characters (which corresponds to the number of files and directories)\n!find test_mini -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.470119Z","iopub.status.idle":"2024-02-15T14:05:35.470472Z","shell.execute_reply.started":"2024-02-15T14:05:35.470300Z","shell.execute_reply":"2024-02-15T14:05:35.470314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TrainingNeuralNetwork\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 5 | Training Neural Network Model for Image Classification</p>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- Keras and other Deep Learning libraries provide pretrained models  \n- These are deep neural networks with efficient architectures(like VGG,Inception,ResNet) that are already trained on datasets like ImageNet  \n- Using these pretrained models, we can use the already learned weights and add few layers on top to finetune the model to our new data  \n- This helps in faster convergance and saves time and computation when compared to models trained from scratch","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n- We currently have a subset of dataset with 3 classes - samosa, pizza and omelette  \n- Use the below code to finetune Inceptionv3 pretrained model","metadata":{}},{"cell_type":"markdown","source":"<a id=\"FinetuneInception1\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:170%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Fine tune Inception Pretrained model using Food 101 dataset</p>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Clear Keras session to release resources\nK.clear_session()\n\n# Number of classes in the dataset\nn_classes = 3\n\n# Image dimensions\nimg_width, img_height = 299, 299\n\n# Directories for training and validation data\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\n\n# Number of samples in the training and validation sets\nnb_train_samples = 2250  # Number of training samples\nnb_validation_samples = 750  # Number of validation samples\n\n# Batch size for training\nbatch_size = 16\n\n# Data augmentation and normalization for training images\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,  # Normalize pixel values to the range [0,1]\n    shear_range=0.2,   # Shear transformation\n    zoom_range=0.2,    # Random zoom\n    horizontal_flip=True)  # Horizontal flip\n\n# Normalization for validation images\ntest_datagen = ImageDataGenerator(rescale=1. / 255)  # Normalize pixel values to the range [0,1]\n\n# Generate batches of training data\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,  # Path to the training data directory\n    target_size=(img_height, img_width),  # Resize images to match the input size of the model\n    batch_size=batch_size,  # Number of samples per batch\n    class_mode='categorical')  # Use categorical labels for multi-class classification\n\n# Generate batches of validation data\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,  # Path to the validation data directory\n    target_size=(img_height, img_width),  # Resize images to match the input size of the model\n    batch_size=batch_size,  # Number of samples per batch\n    class_mode='categorical')  # Use categorical labels for multi-class classification\n\n# Load the pre-trained InceptionV3 model without the top layers\ninception = InceptionV3(weights='imagenet', include_top=False)\n\n# Add custom top layers for fine-tuning\nx = inception.output  # Output tensor of the InceptionV3 model\nx = GlobalAveragePooling2D()(x)  # Global average pooling layer\nx = Dense(128, activation='relu')(x)  # Fully connected layer with ReLU activation\nx = Dropout(0.2)(x)  # Dropout layer for regularization\n\n# Predictions layer with softmax activation for class probabilities\npredictions = Dense(3, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\n# Create the final model with InceptionV3 as the base and custom top layers\nmodel = Model(inputs=inception.input, outputs=predictions)\n\n# Compile the model with SGD optimizer, categorical cross-entropy loss, and accuracy metric\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define callbacks for saving the best model and logging training history\ncheckpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_3class.log')\n\n# Train the model using the training and validation generators\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=nb_train_samples // batch_size,  # Number of batches per epoch\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples // batch_size,  # Number of validation batches per epoch\n    epochs=30,  # Number of training epochs\n    verbose=1,  # Verbosity mode (0=silent, 1=progress bar, 2=one line per epoch)\n    callbacks=[csv_logger, checkpointer])  # List of callbacks for training\n\n# Save the trained model\nmodel.save('model_trained_3class.hdf5')  # Save the model to HDF5 file format\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.471713Z","iopub.status.idle":"2024-02-15T14:05:35.472065Z","shell.execute_reply.started":"2024-02-15T14:05:35.471886Z","shell.execute_reply":"2024-02-15T14:05:35.471912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"IndicesMapping\"></a>\n# <b><span style='color:black'>Step 5.1 |</span><span style='color:#742d0c '> Obtaining the Class Indices Mapping </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Get the class indices mapping for the labels in the training data generator\nclass_map_3 = train_generator.class_indices\n\n# Display the class indices mapping\nclass_map_3\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.473496Z","iopub.status.idle":"2024-02-15T14:05:35.473838Z","shell.execute_reply.started":"2024-02-15T14:05:35.473667Z","shell.execute_reply":"2024-02-15T14:05:35.473681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"VisualizingTrain/Validation\"></a>\n# <b><span style='color:black'>Step 5.2 |</span><span style='color:#742d0c '> Visualizing the Training and Validation Accuracy </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"def plot_accuracy(history, title):\n    \"\"\"\n    Plot training and validation accuracy over epochs.\n    \n    Args:\n    - history: Training history obtained from model training\n    - title: Title of the plot\n    \n    Returns:\n    - None\n    \"\"\"\n    plt.title(title)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\n\ndef plot_loss(history, title):\n    \"\"\"\n    Plot training and validation loss over epochs.\n    \n    Args:\n    - history: Training history obtained from model training\n    - title: Title of the plot\n    \n    Returns:\n    - None\n    \"\"\"\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.475075Z","iopub.status.idle":"2024-02-15T14:05:35.475439Z","shell.execute_reply.started":"2024-02-15T14:05:35.475262Z","shell.execute_reply":"2024-02-15T14:05:35.475281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ModelEvaluation\"></a>\n# <b><span style='color:black'>Step 5.3 |</span><span style='color:#742d0c '> Model Evaluation </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"plot_accuracy(history, 'FOOD101-Inceptionv3')  # Plot training and validation accuracy\nplot_loss(history, 'FOOD101-Inceptionv3')      # Plot training and validation loss\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.476625Z","iopub.status.idle":"2024-02-15T14:05:35.476982Z","shell.execute_reply.started":"2024-02-15T14:05:35.476803Z","shell.execute_reply":"2024-02-15T14:05:35.476817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n   \n- The plots show that the accuracy of the model increased with epochs and the loss has decreased\n- Validation accuracy has been on the higher side than training accuracy for many epochs\nThis could be for several reasons:\n\n-   We used a pretrained model trained on ImageNet which contains data from a variety of classes\n-   Using dropout can lead to a higher validation accuracy","metadata":{}},{"cell_type":"code","source":"%%time\n# Loading the best saved model to make predictions\nK.clear_session()  # Clear Keras session\nmodel_best = load_model('best_model_3class.hdf5', compile=False)  # Load the best saved model\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.478292Z","iopub.status.idle":"2024-02-15T14:05:35.478611Z","shell.execute_reply.started":"2024-02-15T14:05:35.478452Z","shell.execute_reply":"2024-02-15T14:05:35.478465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n   \n- Setting compile=False and clearing the session leads to faster loading of the saved model\n- Withouth the above addiitons, model loading was taking more than a minute!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"PredictingClassLabel\"></a>\n# <b><span style='color:black'>Step 5.4 |</span><span style='color:#742d0c '> Predicting Class Label </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"def predict_class(model, images, show=True):\n    \"\"\"\n    Predict the class label for each image in the given list of image paths.\n    \n    Args:\n    - model: Trained model for making predictions\n    - images: List of image paths\n    - show: Boolean flag to control image display\n    \n    Returns:\n    - None\n    \"\"\"\n    for img in images:\n        img = image.load_img(img, target_size=(299, 299))  # Load image and resize to model's input size\n        img = image.img_to_array(img)                     # Convert image to numpy array\n        img = np.expand_dims(img, axis=0)                 # Add batch dimension\n        img /= 255.                                       # Normalize pixel values\n\n        pred = model.predict(img)                         # Make prediction\n        index = np.argmax(pred)                           # Get the index of the class with the highest probability\n        food_list.sort()                                  # Sort the list of food items\n        pred_value = food_list[index]                     # Get the predicted class label\n        \n        if show:\n            plt.imshow(img[0])                           # Display the image\n            plt.axis('off')\n            plt.title(pred_value)                        # Set title as the predicted class label\n            plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.479498Z","iopub.status.idle":"2024-02-15T14:05:35.479827Z","shell.execute_reply.started":"2024-02-15T14:05:35.479668Z","shell.execute_reply":"2024-02-15T14:05:35.479681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"DownloadImages\"></a>\n# <b><span style='color:black'>Step 5.5 |</span><span style='color:#742d0c '> Downloading Images from the Internet</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Downloading images from internet using the URLs\n!wget -O samosa.jpg http://veggiefoodrecipes.com/wp-content/uploads/2016/05/lentil-samosa-recipe-01.jpg\n!wget -O applepie.jpg https://acleanbake.com/wp-content/uploads/2017/10/Paleo-Apple-Pie-with-Crumb-Topping-gluten-free-grain-free-dairy-free-15.jpg\n!wget -O pizza.jpg http://104.130.3.186/assets/itemimages/400/400/3/default_9b4106b8f65359684b3836096b4524c8_pizza%20dreamstimesmall_94940296.jpg\n!wget -O omelette.jpg https://www.incredibleegg.org/wp-content/uploads/basic-french-omelet-930x550.jpg\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.480792Z","iopub.status.idle":"2024-02-15T14:05:35.481137Z","shell.execute_reply.started":"2024-02-15T14:05:35.480975Z","shell.execute_reply":"2024-02-15T14:05:35.480989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a list of downloaded images\nimages = ['applepie.jpg', 'pizza.jpg', 'omelette.jpg']\n\n# Test the trained model\npredict_class(model_best, images, True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.482525Z","iopub.status.idle":"2024-02-15T14:05:35.482980Z","shell.execute_reply.started":"2024-02-15T14:05:35.482732Z","shell.execute_reply":"2024-02-15T14:05:35.482750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#ed2323>Yes!!! The model got them all right!!</font></h2>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"FinetuneInception\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:170%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Fine tune Inceptionv3 model with 11 classes of data</p>\n⬆️ [Tabel of Contents](#contents_tabel)\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- We trained a model on 3 classes and tested it using new data\n- The model was able to predict the classes of all three test images correctly\n- Will it be able to perform at the same level of accuracy for more classes?\n- FOOD-101 dataset has 101 classes of data\n- Even with fine tuning using a pre-trained model, each epoch was taking more than an hour when all 101 classes of data is used(tried this on both Colab and on a Deep Learning VM instance with P100 GPU on GCP)\n- But to check how the model performs when more classes are included, I'm using the same model to fine tune and train on 11 randomly chosen classes","metadata":{}},{"cell_type":"code","source":"def pick_n_random_classes(n):\n    \"\"\"\n    Select n random food classes from the sorted list of food items.\n    \n    Args:\n    - n: Number of random food classes to select\n    \n    Returns:\n    - List of n randomly selected food classes\n    \"\"\"\n    food_list = []\n    random_food_indices = random.sample(range(len(foods_sorted)), n)  # Sample n random indices\n    for i in random_food_indices:\n        food_list.append(foods_sorted[i])  # Retrieve corresponding food items\n    food_list.sort()  # Sort the list of randomly selected food classes\n    return food_list\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.484503Z","iopub.status.idle":"2024-02-15T14:05:35.484955Z","shell.execute_reply.started":"2024-02-15T14:05:35.484720Z","shell.execute_reply":"2024-02-15T14:05:35.484738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 11  # Number of random food classes to select\nfood_list = pick_n_random_classes(n)  # Select n random food classes\nfood_list = ['apple_pie', 'beef_carpaccio', 'bibimbap', 'cup_cakes', 'foie_gras', 'french_fries', 'garlic_bread', 'pizza', 'spring_rolls', 'spaghetti_carbonara', 'strawberry_shortcake']\nprint(\"These are the randomly picked food classes we will be training the model on...\\n\", food_list)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.486535Z","iopub.status.idle":"2024-02-15T14:05:35.486873Z","shell.execute_reply.started":"2024-02-15T14:05:35.486708Z","shell.execute_reply":"2024-02-15T14:05:35.486722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a message indicating the start of the process\nprint(\"Creating training data folder with new classes...\")\n\n# Call the dataset_mini function to create a new data subset with the selected food classes for training\ndataset_mini(food_list, src_train, dest_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.488547Z","iopub.status.idle":"2024-02-15T14:05:35.488988Z","shell.execute_reply.started":"2024-02-15T14:05:35.488762Z","shell.execute_reply":"2024-02-15T14:05:35.488780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the total number of samples in the train folder\nprint(\"Total number of samples in train folder\")\n\n# Count the number of files and directories in the train_mini folder and print the count\n!find train_mini -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.490535Z","iopub.status.idle":"2024-02-15T14:05:35.490900Z","shell.execute_reply.started":"2024-02-15T14:05:35.490729Z","shell.execute_reply":"2024-02-15T14:05:35.490746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a message indicating the start of the process\nprint(\"Creating test data folder with new classes\")\n\n# Call the dataset_mini function to create a new data subset with the selected food classes for testing\ndataset_mini(food_list, src_test, dest_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.492073Z","iopub.status.idle":"2024-02-15T14:05:35.492414Z","shell.execute_reply.started":"2024-02-15T14:05:35.492229Z","shell.execute_reply":"2024-02-15T14:05:35.492259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a message indicating the start of the process\nprint(\"Total number of samples in test folder\")\n\n# Count the number of files and directories in the test_mini folder and print the count\n!find test_mini -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.493475Z","iopub.status.idle":"2024-02-15T14:05:35.493799Z","shell.execute_reply.started":"2024-02-15T14:05:35.493640Z","shell.execute_reply":"2024-02-15T14:05:35.493654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear any previous sessions to free up memory\nK.clear_session()\n\n# Set the number of classes\nn_classes = n\n\n# Set the dimensions of input images\nimg_width, img_height = 299, 299\n\n# Set the paths for the training and validation data directories\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\n\n# Set the number of training and validation samples\nnb_train_samples = 8250\nnb_validation_samples = 2750\n\n# Set the batch size for training\nbatch_size = 16\n\n# Define data augmentation for training images\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\n# Define data augmentation for validation images\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\n# Generate batches of augmented training and validation data\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# Load the InceptionV3 model pretrained on ImageNet without the top layer\ninception = InceptionV3(weights='imagenet', include_top=False)\n\n# Add custom top layers for classification\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.2)(x)\npredictions = Dense(n, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\n# Create the final model\nmodel = Model(inputs=inception.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define callbacks for model checkpointing and logging\ncheckpointer = ModelCheckpoint(filepath='best_model_11class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_11class.log')\n\n# Train the model\nhistory_11class = model.fit_generator(train_generator,\n                    steps_per_epoch=nb_train_samples // batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples // batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\n# Save the trained model\nmodel.save('model_trained_11class.hdf5')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.495323Z","iopub.status.idle":"2024-02-15T14:05:35.495662Z","shell.execute_reply.started":"2024-02-15T14:05:35.495501Z","shell.execute_reply":"2024-02-15T14:05:35.495515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the class indices for the 11 food classes from the training data generator\nclass_map_11 = train_generator.class_indices\nclass_map_11\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:05:35.499772Z","iopub.status.idle":"2024-02-15T14:05:35.500076Z","shell.execute_reply.started":"2024-02-15T14:05:35.499925Z","shell.execute_reply":"2024-02-15T14:05:35.499938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the accuracy over epochs for the training and validation sets\nplot_accuracy(history_11class, 'FOOD101-Inceptionv3')\n\n# Plot the loss over epochs for the training and validation sets\nplot_loss(history_11class, 'FOOD101-Inceptionv3')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- The plots show that the accuracy of the model increased with epochs and the loss has decreased\n- Validation accuracy has been on the higher side than training accuracy for many epochs\n\n   - This could be for several reasons:\n    \n    \n- We used a pretrained model trained on ImageNet which contains data from a variety of classes\n- Using dropout can lead to a higher validation accuracy \n","metadata":{}},{"cell_type":"code","source":"%%time\n# Clear any previous session and load the best saved model for predictions\nK.clear_session()\nmodel_best = load_model('best_model_11class.hdf5', compile=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downloading images from internet using the URLs\n!wget -O cupcakes.jpg https://www.publicdomainpictures.net/pictures/110000/nahled/halloween-witch-cupcakes.jpg\n!wget -O springrolls.jpg https://upload.wikimedia.org/wikipedia/commons/6/6f/Vietnamese_spring_rolls.jpg\n!wget -O pizza.jpg http://104.130.3.186/assets/itemimages/400/400/3/default_9b4106b8f65359684b3836096b4524c8_pizza%20dreamstimesmall_94940296.jpg\n!wget -O garlicbread.jpg https://c1.staticflickr.com/1/84/262952165_7ba3466108_z.jpg?zz=1\n\n# If you have an image in your local computer and want to try it, uncomment the below code to upload the image files\n\n\n# from google.colab import files\n# image = files.upload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a list of downloaded images and test the trained model\nimages = []\nimages.append('cupcakes.jpg')\nimages.append('pizza.jpg')\nimages.append('springrolls.jpg')\nimages.append('garlicbread.jpg')\npredict_class(model_best, images, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- The model did well even when the number of classes are increased to 11\n- Model training on all 101 classes takes some time\n- It was taking more than an hour for one epoch when the full dataset is used for fine tuning","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ModelExplainability\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 6 | Model Explainability</p>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- Human lives and Technology are blending more and more together\n- The rapid advancements in technology over the past few years can be attributed to how Neural Networks have evolved\n- Neural Networks and Deep Learning are now being used in so many fields and industries - healthcare, finance, retail, automative etc\n- Thanks to the Deep Learning libraries which enable us to develop applications/models with few lines of code, which a decade ago only those with a lot of expertise and research could do\n- All of this calls for the need to understand how neural networks do what they do and how they do it\n- This has led to an active area of research - Neural Network Model Interpretability and Explainability","metadata":{}},{"cell_type":"markdown","source":"![](https://s3-media2.fl.yelpcdn.com/bphoto/7BlRoSOG3AsAWHMPOaG7ng/ls.jpg)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- Neural Networks learn incrementally\n- How does a neural network know what is in the image and how does it conclude that its a dog?\n- The best analogy to understand the incremental learning of the model here is to think about how we would hand sketch the dog\n- You can't start right away by drawing eyes, nose, snout etc\n- To have any of those dogly features, you need a lot of edges and curves\n- You start with edges/lines, put many of them together\n- Use edges with curves to sketch patterns\n- The patterns with more finer details will help us draw the visible features of a dog like eyes, ears, snout etc\n- Neural networks adopt a very similar process when they are busy detecting what's in the provided data examples","metadata":{}},{"cell_type":"markdown","source":"![](https://images.deepai.org/publication-preview/visualizing-and-understanding-convolutional-networks-page-4-medium.jpg)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* The above image is taken from the paper - [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)\n* The image contains the features of a trained model along with the kind of objects they would detect\n* In the first row and first column, we have a grid of edge detecting features in layer 1 and some curve detectors in layer 2 in the 2nd column\n* The last column in 1st row are the kind of objects that get detected using those curvy features\n* With layer three in 2nd row, the model starts looking for patterns with edges and curves\n* The second column in second row contains examples of patterns that are detected in layer 3 of the model\n* With layer 4, the model starts detecting parts of object specific features and in layer 5 the model knows what's in the image","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* Using feature visualization, we can know what a neural network layer and its features are looking for\n* Using attribution, we can understand how the features impact the output and what regions in the image led the model to the generated output","metadata":{}},{"cell_type":"markdown","source":"<a id=\"EvaluateModel\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 7 | Evaluating the Model</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"LoadingtheSavedModel\"></a>\n# <b><span style='color:black'>Step 7.1 |</span><span style='color:#742d0c '> Loading the Saved Model and a Test Image</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n\n","metadata":{}},{"cell_type":"code","source":"# Load the saved model trained with 3 classes\nK.clear_session()\nprint(\"Loading the model..\")\nmodel = load_model('best_model_3class.hdf5', compile=False)\nprint(\"Done!\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"SummaryModel\"></a>\n# <b><span style='color:black'>Step 7.2 |</span><span style='color:#742d0c '> Summary of the Model </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"DefiningHelperFunctions\"></a>\n# <b><span style='color:black'>Step 7.3 |</span><span style='color:#742d0c '> Defining Helper Functions</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"def deprocess_image(x):\n    # Normalize tensor: center on 0., ensure standard deviation is 0.1\n    x -= x.mean()\n    x /= (x.std() + 1e-5)\n    x *= 0.1\n\n    # Clip values to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # Convert to RGB array\n    x *= 255\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GeneratingPatternFunction\"></a>\n# <b><span style='color:black'>Step 7.4 |</span><span style='color:#742d0c '> Generating Pattern Function\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"def generate_pattern(layer_name, filter_index, size=150):\n    # Build a loss function that maximizes the activation\n    # of the nth filter of the layer considered.\n    layer_output = model.get_layer(layer_name).output\n    loss = K.mean(layer_output[:, :, :, filter_index])\n\n    # Compute the gradient of the input picture wrt this loss\n    grads = K.gradients(loss, model.input)[0]\n\n    # Normalization trick: we normalize the gradient\n    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n\n    # This function returns the loss and grads given the input picture\n    iterate = K.function([model.input], [loss, grads])\n    \n    # We start from a gray image with some noise\n    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n\n    # Run gradient ascent for 40 steps\n    step = 1.\n    for i in range(40):\n        loss_value, grads_value = iterate([input_img_data])\n        input_img_data += grads_value * step\n        \n    img = input_img_data[0]\n    return deprocess_image(img)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GettingActivationsFunction\"></a>\n# <b><span style='color:black'>Step 7.5 |</span><span style='color:#742d0c '> Getting Activations Function\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"def get_activations(img, model_activations):\n    \"\"\"\n    Get activations of a model for a given image.\n\n    Args:\n    img (str): Path to the image file.\n    model_activations (Model): Model object to get activations from.\n\n    Returns:\n    numpy.ndarray: Activations produced by the model.\n    \"\"\"\n    # Load and preprocess the image\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img /= 255. \n    # Visualize the image\n    plt.imshow(img[0])\n    plt.show()\n    # Get activations\n    return model_activations.predict(img)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ShowingActivationsFunction\"></a>\n# <b><span style='color:black'>Step 7.6 |</span><span style='color:#742d0c '> Showing Activations Function\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"def show_activations(activations, layer_names):\n    \"\"\"\n    Display feature maps for each layer.\n\n    Args:\n    activations (list of numpy.ndarray): List of activation tensors for each layer.\n    layer_names (list of str): Names of the layers.\n\n    Returns:\n    None\n    \"\"\"\n    images_per_row = 16\n\n    # Loop through each layer\n    for layer_name, layer_activation in zip(layer_names, activations):\n        # Number of features in the feature map\n        n_features = layer_activation.shape[-1]\n\n        # Size of the feature map\n        size = layer_activation.shape[1]\n\n        # Number of columns for visualization\n        n_cols = n_features // images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n        # Tile each filter into a big horizontal grid\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0, :, :, col * images_per_row + row]\n                # Post-process the feature to make it visually palatable\n                channel_image -= channel_image.mean()\n                channel_image /= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n\n        # Display the grid\n        scale = 1. / size\n        plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\nCheck how many layers are in the trained model(this includes the 1st input layer as well)\n","metadata":{}},{"cell_type":"code","source":"len(model.layers)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n> 1. * Can we visualize the outputs of all the layers?\n* Yes, we can. But that gets too tedious\n* So, let's choose a few layers to visualize","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ExtractingIntermediateLayerActivations\"></a>\n# <b><span style='color:black'>Step 7.7 |</span><span style='color:#742d0c '> Extracting Intermediate Layer Activations\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# We start with index 1 instead of 0, as input layer is at index 0\nlayers = [layer.output for layer in model.layers[1:11]]\n# We now initialize a model which takes an input and outputs the above chosen layers\nactivations_output = models.Model(inputs=model.input, outputs=layers)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\nAs seen below, the 10 chosen layers contain 3 convolution, 3 batch normalization, 3 activation and 1 max pooling layers\n","metadata":{}},{"cell_type":"code","source":"layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n>  * Get the names of all the selected layers","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ExtractingLayerNamesforIntermediateActivations\"></a>\n# <b><span style='color:black'>Step 7.8 |</span><span style='color:#742d0c '> Extracting Layer Names for Intermediate Activations\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Initialize an empty list to store the names of the layers\nlayer_names = []\n\n# Iterate through the layers of the model starting from index 1 and ending at index 10 (inclusive)\nfor layer in model.layers[1:11]:\n    # Append the name of each layer to the list\n    layer_names.append(layer.name)\n\n# Print the list of layer names\nprint(layer_names)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \nProvide an input to the model and get the activations of all the 10 chosen layers\n","metadata":{}},{"cell_type":"code","source":"# Define the filename of the image\nfood = 'applepie.jpg'\n\n# Get the activations of the model for the specified image using the defined activations_output model\nactivations = get_activations(food, activations_output)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n**activations** contain the outputs of all the 10 layers which can be plotted and visualized","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\nVisualize the activations of intermediate layers from layer 1 to 10","metadata":{}},{"cell_type":"code","source":"# Visualize the activations of the model for the specified image using the defined layer names\nshow_activations(activations, layer_names)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n* What we see in the above plots are the activations or the outputs of each of the 11 layers we chose \n* The activations or the outputs from the 1st layer(conv2d_1) don't lose much information of the original input\n* They are the results of applying several edge detecting filters on the input image**\n* With each added layer, the activations lose visual/input information and keeps building on the class/ouput information\n* As the depth increases, the layers activations become less visually interpretabale and more abstract\n* By doing so, they learn to detect more specific features of the class rather than just edges and curves\n* We plotted just 10 out of 314 intermediate layers. We already have in these few layers, activations which are blank/sparse(for ex: the 2 blank activations in the layer activation_1)\n* These blank/sparse activations are caused when any of the filters used in that layer didn't find a matching pattern in the input given to it\n* By plotting more layers(specially those towards the end of the network), we can observe more of these sparse activations and how the layers get more abstract\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"GettingtheActivationsforaDifferentInput/Food\"></a>\n# <b><span style='color:black'>Step 7.9 |</span><span style='color:#742d0c '> Getting the Activations for a Different Input / Food\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Get the activations for the specified image using the defined layer names\nactivations = get_activations(food, activations_output)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the activations for the specified image using the defined layer names\nshow_activations(activations, layer_names)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n* The feature maps in the above activations are for a different input image\n* We see the same patterns discussed for the previous input image\n* It is interesting to see the blank/sparse activations in the same layer(activation_1) and for same filters when a different image is passed to the network\n* Remember we used a pretrained Inceptionv3 model. All the filters that are used in different layers come from this pretrained model","metadata":{}},{"cell_type":"markdown","source":"<a id=\"LookintotheSparseActivationsintheLayerActivation_1/Food\"></a>\n# <b><span style='color:black'>Step 7.10 |</span><span style='color:#742d0c '> Look into the Sparse Activations in the Layer Activation_1\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* We have two blank/sparse activations in layer 6\n* Below cell displays one of the sparse activations","metadata":{}},{"cell_type":"code","source":"# Get the index of activation_1 layer which has sparse activations\nind = layer_names.index('activation_1')\nsparse_activation = activations[ind]\n# Select the activation values of a specific filter\na = sparse_activation[0, :, :, 13]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all(np.isnan(a[j][k]) for j in range(a.shape[0]) for k in range(a.shape[1]))\n#This line checks if all elements in the array a are NaN.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* We can see that the activation has all nan values(it was all zeros when executed outside Kaggle, Im yet to figure out why its showing all nan values here\n \n* To know why we have all zero/nan values for this activation, lets visualize the activation at same index 13 from previous layer","metadata":{}},{"cell_type":"code","source":"# Get the index of batch_normalization_1 layer which has sparse activations\nind = layer_names.index('batch_normalization_1')\n# Extract sparse activations from the layer\nsparse_activation = activations[ind]\n# Select activations for the 14th filter\nb = sparse_activation[0, :, :, 13]\n# Print the sparse activations\nb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* All the values in the above activation map from the layer batch_normalization_1 are negative\n* This activation in batch_normalization_1 is passed to the next layer activation_1 as input\n* As the name says, activation_1 is an activation layer and ReLu is the activation function used\n* ReLu takes an input value, returns 0 if its negative, the value otherwise\n* Since the input to activation array contains all negative values, the activation layer fills its activation map with all zeros for the index\n* Now we know why we have those 2 sparse activations in activation_1 layer","metadata":{}},{"cell_type":"markdown","source":"<a id=\"VisualizationofConvolutionalLayerActivations\"></a>\n# <b><span style='color:black'>Step 7.11 |</span><span style='color:#742d0c '> Visualization of Convolutional Layer Activations\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Extract activations for the first three convolutional layers\nfirst_convlayer_activation = activations[0]\nsecond_convlayer_activation = activations[3]\nthird_convlayer_activation = activations[6]\n\n# Visualize the activations for each layer\nf, ax = plt.subplots(1, 3, figsize=(10, 10))\n\n# Plot activations for the first convolutional layer\nax[0].imshow(first_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[0].axis('OFF')\nax[0].set_title('Conv2d_1')\n\n# Plot activations for the second convolutional layer\nax[1].imshow(second_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[1].axis('OFF')\nax[1].set_title('Conv2d_2')\n\n# Plot activations for the third convolutional layer\nax[2].imshow(third_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[2].axis('OFF')\nax[2].set_title('Conv2d_3')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GeneratingClassActivationMap\"></a>\n# <b><span style='color:black'>Step 7.12 |</span><span style='color:#742d0c '> Generating Class Activation Map (CAM)\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"def get_attribution(food):\n    \"\"\"\n    Generate class activation map for a given food image using Grad-CAM technique.\n\n    Args:\n    food (str): Path to the food image.\n\n    Returns:\n    numpy.ndarray: Predictions made by the model for the input image.\n    \"\"\"\n    # Load and preprocess the input image\n    img = image.load_img(food, target_size=(299, 299))\n    img = image.img_to_array(img) \n    img /= 255. \n\n    # Display the input image\n    f, ax = plt.subplots(1, 3, figsize=(15, 15))\n    ax[0].imshow(img)\n    ax[0].set_title(\"Input Image\")\n\n    # Expand the dimensions and predict the class probabilities\n    img = np.expand_dims(img, axis=0) \n    preds = model.predict(img)\n    class_id = np.argmax(preds[0])\n\n    # Get the class output and last convolutional layer\n    class_output = model.output[:, class_id]\n    last_conv_layer = model.get_layer(\"mixed10\")\n    \n    # Calculate gradients and pooled gradients\n    grads = K.gradients(class_output, last_conv_layer.output)[0]\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img])\n\n    # Generate heatmap\n    for i in range(2048):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n    ax[1].imshow(heatmap)\n    ax[1].set_title(\"Heat map\")\n    \n    # Overlay heatmap on the original image\n    act_img = cv2.imread(food)\n    heatmap = cv2.resize(heatmap, (act_img.shape[1], act_img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed = cv2.addWeighted(act_img, 0.6, heatmap, 0.4, 0)\n    cv2.imwrite('classactivation.png', superimposed)\n    img_act = image.load_img('classactivation.png', target_size=(299, 299))\n    ax[2].imshow(img_act)\n    ax[2].set_title(\"Class Activation\")\n    plt.show()\n    return preds\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Showing the class map..\")\nprint(class_map_3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GettingAttributionandDisplaySoftmaxPredictions\"></a>\n# <b><span style='color:black'>Step 7.13 |</span><span style='color:#742d0c '> Getting Attribution and Display Softmax Predictions\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Get attribution for the image 'applepie.jpg' and display the softmax predictions\npred = get_attribution('applepie.jpg')\nprint(\"Here are softmax predictions:\", pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get attribution and display softmax predictions for the pizza image\npred = get_attribution('pizza.jpg')\nprint(\"Here are softmax predictions:\", pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* We can see how the heat map is different for a different image i.e the model looks for a totally different features/regions if it has to classify it as a pizza","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n- Lets see if we can break the model or see what it does when we surpise it with different data!","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n* We trained our model to perform multi class classification and it seems to be doing well with >95% of accuracy\n* What will the model do when we give it an image which has more than one object that model is trained to classify?","metadata":{}},{"cell_type":"markdown","source":"<a id=\"DownloadingImagesfromURLs\"></a>\n# <b><span style='color:black'>Step 7.14 |</span><span style='color:#742d0c '> Downloading Images from URLs\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Downloading images from internet using the URLs\n!wget -O piepizza.jpg https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/piepizza.jpg\n!wget -O piepizzas.png https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/piepizzas.png\n!wget -O pizzapie.jpg https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/pizzapie.jpg\n!wget -O pizzapies.png https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/pizzapies.png\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"LoadingandRetrievingActivations\"></a>\n# <b><span style='color:black'>Step 7.15 |</span><span style='color:#742d0c '> Loading and Retrieving Activations\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"food = 'piepizza.jpg'\nactivations = get_activations(food,activations_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_activations(activations, layer_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_attribution('piepizza.jpg')\nprint(\"Here are softmax predictions..\",pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n\n* Given an image with pizza and applepie, the model thinks its a pizza with 75.4% confidence and an applie pie with 18% confidence\n* Now let's flip the image vertically and see what the model does","metadata":{}},{"cell_type":"code","source":"food = 'pizzapie.jpg'\nactivations = get_activations(food,activations_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_attribution('pizzapie.jpg')\nprint(\"Here are softmax predictions..\",pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n\n* Well, the model flipped its output too!\n* The model now thinks its an apple pie with 49.7% confidence and a pizza with 31.9%","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#ed2323>More surprise data to the model...</font></h2>\n","metadata":{}},{"cell_type":"code","source":"food = 'pizzapies.png'\nactivations = get_activations(food,activations_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_attribution('pizzapies.png')\nprint(\"Here are softmax predictions..\",pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n\n* This time it's applie pie with 73% and a pizza with 19% confidence\n* Let's try one last horizontal flip, this is the last really!","metadata":{}},{"cell_type":"code","source":"food = 'piepizzas.png'\nactivations = get_activations(food,activations_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_attribution('piepizzas.png')\nprint(\"Here are softmax predictions..\",pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n* No surprise from model this time. We flipped the image but the model didnt flip its output\n* It's an apple pie again with 52% confidence","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n### Further Improvements\n* Try more augmentation on test images\n* Fine tune the model on the entire dataset(for a few epochs atleast)\n* Play with hyper parameters, their values and see how it impacts model performance\n* There is currently no implementation to handle out of distribution / no class scenario. Can try below methods:\n    \n  * Set a threshold for the class with highest score. When model gives prediction score below the threshold for its top prediction, the prediction can be classified as NO-CLASS / UNSEEN\n  * Add a new class called NO-CLASS, provide data from different classes other than those in the original dataset. This way the model also learns how to classify totally unseen/unrelated data\n  * I am yet to try these methods and not sure about the results\n* Recently published paper - [Rethinking ImageNet Pretraining](https://arxiv.org/abs/1811.08883 ), claims that training from random initialization instead of using pretrained weights is not only robust but also gives comparable results\n* Pre-trained models are surely helpful. They save a lot of time and computation. Yet, that shouldn't be the reason to not try to train a model from scratch\n* Time taking yet productive experiment would be to try and train a model on this dataset from scratch\n* Do more experiments with Model Interpretability and see what can be observed","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n### Feedback\n* Did you find any issues with the above code or have any suggestions or corrections?**\n* There must be many ways to improve the model, its architecture, hyperparameters..**\n* Please do let me know!\n* [Github](https://github.com/Engr-Umer)\n* [Linkedin](https://www.linkedin.com/in/muhammad-umer-mujahid/)","metadata":{}},{"cell_type":"markdown","source":"<h1 align=\"left\"><font color=#ed2323>Love you all and keep supporting</font></h1>","metadata":{}}]}